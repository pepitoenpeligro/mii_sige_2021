---
title: "P2 - Deep Learning con conjunto de datos Fakeddit"
author: 
  - José Antonio Córdoba Gómez
  - Fernando Izquierdo Romera
date: "4/25/2021"
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
    number_sections: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(keras)
library(caret)
library(mice)
library(rpart.plot)
library(scales)
set.seed(0)
```

# Carga de datos

De forma inicial, probamos que podemos cargar alguna imagen del conjunto mediano **medium10000_sixClasses**.

```{r, fig.align='center', echo=FALSE}
img_sample <- image_load(path = './data/images/medium10000_sixClasses/test/1/3zaywx.jpg', 
                         target_size = c(150, 150))

img_sample_array <- array_reshape(image_to_array(img_sample), 
                                  c(1, 150, 150, 3))

plot(as.raster(img_sample_array[1,,,] / 255))
```

# Analisis Explotatorio de Datos (EDA)

Cargamos el conjunto de imágenes

```{r}
dataset_dir           <- './data/images/medium10000_twoClasses'
train_images_dir      <- paste0(dataset_dir, '/train')
val_images_dir        <- paste0(dataset_dir, '/val')
test_images_dir       <- paste0(dataset_dir, '/test')
comments_file          <- './data/comments/all_comments.tsv'
```

Cargamos el conjunto de metadatos

```{r}
metadata_train <- read_tsv(paste0(train_images_dir, "/multimodal_train.tsv"))
metadata_train <- metadata_train %>%
  mutate(created_at = as.POSIXct(created_utc, origin="1970-01-01")) %>%
  select(-one_of('created_utc')) %>%
  mutate(class = ifelse(`2_way_label` == 0, 'Disinformation', 'Other'))

```

```{r, eval=FALSE}
comments <- read_tsv(comments_file)
```
Podemos ver en el conjunto de comentarios que existen valores perdidos y podemos comprobar como la mayor parte de los valores perdidos se concentran las dimensiones "isTopLevel" y "ups".
```{r,  eval=FALSE}
summary(comments)
```

Y omitimos los valores pedidos del conjunto de datos de los comentarios:

```{r, eval=FALSE}
comments <- comments %>%
  drop_na()
```

Combinamos los metadatos con los comentarios haciendo un **inner_join**

```{r, eval=FALSE}
metadata_train_comments <- left_join(x = metadata_train, y = comments, 
                                     by = c("id" = "submission_id"),
                                     keep = FALSE, suffix = c('.publication', '.comment'))

metadata_train_comments
```

Seleccionamos las clases e investigamos como se distribuyen para analizar su balanceamiento.

```{r, eval=FALSE}
data_binary <- metadata_train %>%
  select(-one_of('3_way_label', '6_way_label', '2_way_label'))
```

Tal y como podemos apreciar en la siguiente figura, las clases se encuentran desbalanceadas gravemente. Existen muchas más instancias de la clase 'Desinformación' que de cualquier otra clase.

```{r, eval=FALSE}
table(data_binary$class)

ggplot(data_binary) +
  geom_histogram(aes(x = class, fill = class), stat = 'count')
```

Como hemos visto anteriormente el problema del balanceo de clases podría concluirnos a un mal proceso de entrenamiento por tanto, decidimos desde esto punto soluciar este desbalanceo aplicando la técnica de *downsampling*.

```{r, eval=FALSE}
data_factor <- data_binary
data_factor$class <- as.factor(data_factor$class)
predictors <- select(data_factor, -class) 
data_balanced <- downSample(x = predictors,
                             y=data_factor$class, yname='class')
```

```{r, eval=FALSE}
table(data_balanced$class)

ggplot(data_balanced) +
  geom_histogram(aes(x = class, fill = class), stat = 'count')
```





A continuación mostramos la evolución temporal de la densidad de los hilos con desinformación, podemos notar como ha ido creciendo de forma notoria la desinformación.

```{r, eval=FALSE}
ggplot(metadata_train, aes(x = created_at)) +
  geom_histogram(aes(fill = class))
```

A continuación vamos a tratar de avergiguar los autores que más desinformación propagan en este conjunto de datos. Podemos observar como usuarios como @all-top-today_SS o @ApiContraption son los usuarios que más desinformación ha propagado en en toda la serie temporal.

```{r, eval=FALSE}
plotdata <- data_binary %>%
  filter(class == "Disinformation") %>%
  count(author) %>%
  slice_max(n = 25, order_by = n, with_ties = FALSE)
  
ggplot(plotdata) +
  geom_bar(aes(x = author, y = n), stat = 'identity') +
  coord_flip()
```

A continuación vamos a estudiar los dominios qeu más desinformación contienen:

```{r, eval=FALSE}
plotdata <- data_binary %>%
  filter(class == "Disinformation") %>%
  count(domain) %>%
  slice_max(n = 25, order_by = n, with_ties = FALSE)
  
ggplot(plotdata) +
  geom_bar(aes(x = domain, y = n), stat = 'identity') +
  coord_flip()
```






Ahora, vamos a extraer las características de los comentarios como la longitud del título, a aparición de emojis y de ciertos dígitos para analizar distribución de densidad de la desinformación, respecto a la longitud del título.

```{r, eval=FALSE}
data_binary_extended <- data_binary %>%
  mutate(title_text_exclamations = str_count(title, "!")) %>%
  mutate(title_text_caps = str_count(title, "[A-Z]")) %>%
  mutate(title_text_digits = str_count(title, "[0-9]")) %>%
  mutate(title_text_emojis = str_count(title, '[\U{1F300}-\U{1F6FF}]')) %>%
  mutate(title_text_emoji_flag = str_count(title, '\U{1F1FA}|\U{1F1F8}]'))
```

En el siguiente diagrama podemos ver dicha distribución de densidad y podemos notar que los comentarios clasificados como desinformación se agrupan en la zona de títulos con longitud baja.

```{r, eval=FALSE}
ggplot(data_binary_extended) + 
  geom_density(aes(x=title_text_caps, color=class, fill=class), alpha = 0.5)  +
  scale_x_continuous(trans="log10")
```

Ahora vamos a extraer características de los comentarios:

```{r, eval=FALSE}
data_binary_comments <- metadata_train_comments %>%
  select(-one_of('3_way_label', '6_way_label', '2_way_label'))

data_binary_comments_extended <- data_binary_comments %>%
  mutate(body_text_exclamations = str_count(body, "!")) %>%
  mutate(body_text_caps = str_count(body, "[A-Z]")) %>%
  mutate(body_text_digits = str_count(body, "[0-9]")) %>%
  mutate(body_text_emojis = str_count(body, '[\U{1F300}-\U{1F6FF}]')) %>%
  mutate(body_text_emoji_flag = str_count(body, '\U{1F1FA}|\U{1F1F8}]'))
```

Y podemos observar como en la distribución de la densidad de la longitud del cuerpo del comentario, no nos revela información útil ya que la distribución es equitativa en toda la serie temporal.

```{r, eval=FALSE}
ggplot(data_binary_comments_extended) + 
  geom_density(aes(x=body_text_caps, color=class, fill=class), alpha = 0.5)  +
  scale_x_continuous(trans="log10")
```

A continuación mostramos la distribución de la densidad de desinformación con respecto a la cantidad de emojis dentro del cuerpo del comentario, y podemos ver que aunque está prácticamente a la par, es cierto que un uso medio alto de emojis se encuentra en una tasa de desinformación más alta.

```{r, eval=FALSE}
ggplot(data_binary_comments_extended) + 
  geom_density(aes(x=body_text_emojis, color=class, fill=class), alpha = 0.5)  +
  scale_x_continuous(trans="log10")
```




























# Aprendizaje automático

## Modelo Básico



```{r}
train_images_generator <- image_data_generator(rescale = 1/255)
val_images_generator   <- image_data_generator(rescale = 1/255)
test_images_generator  <- image_data_generator(rescale = 1/255)
```

Vamos a aprovechr la funcionalidad de flujo para poder introduciur los datos al modelo.

```{r}
train_generator_flow_1 <- flow_images_from_directory(
  directory = train_images_dir,
  generator = train_images_generator,
  class_mode = 'categorical',
  batch_size = 128,
  target_size = c(64, 64)         # (w x h) --> (64 x 64)
)

validation_generator_flow_1 <- flow_images_from_directory(
  directory = val_images_dir,
  generator = val_images_generator,
  class_mode = 'categorical',
  batch_size = 128,
  target_size = c(64, 64)         # (w x h) --> (64 x 64)
)

test_generator_flow_1 <- flow_images_from_directory(
  directory = test_images_dir,
  generator = test_images_generator,
  class_mode = 'categorical',
  batch_size = 128,
  target_size = c(64, 64)         # (w x h) --> (64 x 64)
)
```

Tomamos un modelo convolucional
 
```{r}
model_1 <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32,  kernel_size = c(3, 3), activation = "relu", input_shape = c(64, 64, 3)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64,  kernel_size = c(3, 3), activation = "relu") %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(3, 3), activation = "relu") %>% layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 2, activation = "softmax")


```

Compilamos el modelo

```{r}
model_1 %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```



```{r}
specify_decimal <- function(x, k) trimws(format(round(x, k), nsmall=k))
```


```{r}

NEXEC = 5
loss <- 0
precision <- 0
time <- 0

for (i in 1:NEXEC){
 start_model_1 <- Sys.time()
  history <- model_1 %>% 
  fit_generator(
    generator = train_generator_flow_1, 
    validation_data = validation_generator_flow_1,
    steps_per_epoch = 10,
    epochs = 10
  )
  
  
  end_model_1 <- Sys.time()
  plot(history)
  
  metrics <- model_1 %>% 
    evaluate_generator(test_generator_flow_1, steps = 5)
  
  time <- time + (end_model_1 - start_model_1)
  precision <- precision + as.numeric(metrics[2])
  loss <- loss + as.numeric(metrics[1])
  message("Iteracion: ",i )
  message("\t Tiempo: ", specify_decimal((end_model_1 - start_model_1), 2))
  message("\tPrecisión: ", specify_decimal(as.numeric(metrics[2]), 2))
  message("\tPerdida: ", specify_decimal(as.numeric(metrics[1]), 2))
}

time <- time / NEXEC
precision <- precision /NEXEC
loss <- loss / NEXEC

message("Total: ",i )
message("\t Tiempo: ", specify_decimal(time,2))
message("\tPrecisión: ", specify_decimal(as.numeric(precision),2))
message("\tPerdida: ", specify_decimal(as.numeric(loss),2))

```


























## Otro modelo

```{r}
train_generator_flow_2 <- flow_images_from_directory(
  directory = train_images_dir,
  generator = train_images_generator,
  class_mode = 'categorical',
  batch_size = 125,
  target_size = c(64, 64)         # (w x h) --> (64 x 64)
)

validation_generator_flow_2 <- flow_images_from_directory(
  directory = val_images_dir,
  generator = val_images_generator,
  class_mode = 'categorical',
  batch_size = 128,
  target_size = c(64, 64)         # (w x h) --> (64 x 64)
)

test_generator_flow_2 <- flow_images_from_directory(
  directory = test_images_dir,
  generator = test_images_generator,
  class_mode = 'categorical',
  batch_size = 128,
  target_size = c(64, 64)         # (w x h) --> (64 x 64)
)
```


```{r}
model_2 <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32,  kernel_size = c(5, 5), activation = "relu", input_shape = c(64, 64, 3)) %>%
  layer_conv_2d(filters = 32,  kernel_size = c(5, 5), activation = "relu") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(5, 5), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(5, 5), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 256, activation = "relu") %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 2, activation = "softmax")
```


```{r}

model_2 %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

```



```{r}

NEXEC = 5
loss <- 0
precision <- 0
time <- 0

for (i in 1:NEXEC){
 start_model_2 <- Sys.time()
  history <- model_2 %>% 
  fit_generator(
    generator = train_generator_flow_2, 
    validation_data = validation_generator_flow_2,
    steps_per_epoch = 10,
    epochs = 10
  )
  
  
  end_model_2 <- Sys.time()
  plot(history)
  
  metrics <- model_2 %>% 
    evaluate_generator(test_generator_flow_1, steps = 5)
  
  time <- time + (end_model_2 - start_model_2)
  precision <- precision + as.numeric(metrics[2])
  loss <- loss + as.numeric(metrics[1])
  message("Iteracion: ",i )
  message("\t Tiempo: ", specify_decimal((end_model_2 - start_model_2), 2))
  message("\tPrecisión: ", specify_decimal(as.numeric(metrics[2]), 2))
  message("\tPerdida: ", specify_decimal(as.numeric(metrics[1]), 2))
}

time <- time / NEXEC
precision <- precision /NEXEC
loss <- loss / NEXEC

message("Total: ",i )
message("\t Tiempo: ", specify_decimal(time,2))
message("\tPrecisión: ", specify_decimal(as.numeric(precision),2))
message("\tPerdida: ", specify_decimal(as.numeric(loss),2))
```


























## 3. Mejoda de nuestra red

```{r}

train_generator_flow_3 <- flow_images_from_directory(
  directory = train_images_dir,
  generator = train_images_generator,
  class_mode = 'categorical',
  batch_size = 128,
  target_size = c(64, 64)         # (w x h) --> (64 x 64)
)

validation_generator_flow_3 <- flow_images_from_directory(
  directory = val_images_dir,
  generator = val_images_generator,
  class_mode = 'categorical',
  batch_size = 128,
  target_size = c(64, 64)         # (w x h) --> (64 x 64)
)

test_generator_flow_3 <- flow_images_from_directory(
  directory = test_images_dir,
  generator = test_images_generator,
  class_mode = 'categorical',
  batch_size = 128,
  target_size = c(64, 64)         # (w x h) --> (64 x 64)
)

```

# Cambiamos la activacion de relu a sigmoide 

```{r}

model_3 <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32,  kernel_size = c(5, 5), activation = "sigmoid", input_shape = c(64, 64, 3)) %>%
  layer_conv_2d(filters = 32,  kernel_size = c(5, 5), activation = "sigmoid") %>% 
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(5, 5), activation = "sigmoid") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(5, 5), activation = "sigmoid") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dense(units = 256, activation = "sigmoid") %>%
  layer_dense(units = 512, activation = "sigmoid") %>%
  layer_dense(units = 2, activation = "softmax")
```


```{r}
model_3 %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```


```{r}
NEXEC = 5
loss <- 0
precision <- 0
time <- 0

for (i in 1:NEXEC){
 start_model_3 <- Sys.time()
  history <- model_3 %>% 
  fit_generator(
    generator = train_generator_flow_3, 
    validation_data = validation_generator_flow_3,
    steps_per_epoch = 10,
    epochs = 10
  )
  
  
  end_model_3 <- Sys.time()
  plot(history)
  
  metrics <- model_3 %>% 
    evaluate_generator(test_generator_flow_3, steps = 5)
  
  time <- time + (end_model_3 - start_model_3)
  precision <- precision + as.numeric(metrics[2])
  loss <- loss + as.numeric(metrics[1])
  message("Iteracion: ",i )
  message("\t Tiempo: ", specify_decimal((end_model_3 - start_model_3), 2))
  message("\tPrecisión: ", specify_decimal(as.numeric(metrics[2]), 2))
  message("\tPerdida: ", specify_decimal(as.numeric(metrics[1]), 2))
}

time <- time / NEXEC
precision <- precision /NEXEC
loss <- loss / NEXEC

message("Total: ",i )
message("\t Tiempo: ", specify_decimal(time,2))
message("\tPrecisión: ", specify_decimal(as.numeric(precision),2))
message("\tPerdida: ", specify_decimal(as.numeric(loss),2))

```







## Aumento de imagenes y batch normalization y mejoras del apredizaje



Añadimos el dropoutSuponiendo que el mejor ha sio el sigmodie.




Añadimos imageens al conjunto de entrenamiento aplicandole leves variaciones en el zoom, el rango y la rotacion principalmente y haciendole la función espejo.
```{r}
train_images_generator <- image_data_generator(
  rescale = 1/255,
  rotation_range = 28,
  width_shift_range = 0.2,
  height_shift_range = 0.2,
  shear_range = 0.15,
  zoom_range = 0.15,
  horizontal_flip = TRUE,
  fill_mode = "nearest"
)

val_images_generator   <- image_data_generator(rescale = 1/255)
test_images_generator  <- image_data_generator(rescale = 1/255)


train_generator_flow_4 <- flow_images_from_directory(
  directory = train_images_dir,
  generator = train_images_generator,
  class_mode = 'categorical',
  batch_size = 128,
  target_size = c(64, 64)         # (w x h) --> (64 x 64)
)

validation_generator_flow_4 <- flow_images_from_directory(
  directory = val_images_dir,
  generator = val_images_generator,
  class_mode = 'categorical',
  batch_size = 128,
  target_size = c(64, 64)         # (w x h) --> (64 x 64)
)

test_generator_flow_4 <- flow_images_from_directory(
  directory = test_images_dir,
  generator = test_images_generator,
  class_mode = 'categorical',
  batch_size = 128,
  target_size = c(64, 64)         # (w x h) --> (64 x 64)
)

```


Añadimos dropout y batch_normalization
```{r}
model_4 <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32,  kernel_size = c(5, 5), activation = "sigmoid", input_shape = c(64, 64, 3)) %>%
  layer_conv_2d(filters = 32,  kernel_size = c(5, 5), activation = "sigmoid") %>% 
  layer_batch_normalization(epsilon = 0.01) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(5, 5), activation = "sigmoid") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(5, 5), activation = "sigmoid") %>%
  layer_batch_normalization(epsilon = 0.01) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 256, activation = "sigmoid") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 512, activation = "sigmoid") %>%
  layer_dense(units = 2, activation = "softmax")

```


```{r}
model_4 %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)
```



```{r}
NEXEC = 5
loss <- 0
precision <- 0
time <- 0

for (i in 1:NEXEC){
 start_model_4 <- Sys.time()
  history <- model_4 %>% 
  fit_generator(
    generator = train_generator_flow_4, 
    validation_data = validation_generator_flow_4,
    steps_per_epoch = 10,
    epochs = 10
  )
  
  
  end_model_4 <- Sys.time()
  plot(history)
  
  metrics <- model_4 %>% 
    evaluate_generator(test_generator_flow_4, steps = 5)
  
  time <- time + (end_model_4 - start_model_4)
  precision <- precision + as.numeric(metrics[2])
  loss <- loss + as.numeric(metrics[1])
  message("Iteracion: ",i )
  message("\t Tiempo: ", specify_decimal((end_model_4 - start_model_4), 2))
  message("\tPrecisión: ", specify_decimal(as.numeric(metrics[2]), 2))
  message("\tPerdida: ", specify_decimal(as.numeric(metrics[1]), 2))
}

time <- time / NEXEC
precision <- precision /NEXEC
loss <- loss / NEXEC

message("Total: ",i )
message("\t Tiempo: ", specify_decimal(time,2))
message("\tPrecisión: ", specify_decimal(as.numeric(precision),2))
message("\tPerdida: ", specify_decimal(as.numeric(loss),2))

```
















```{r}
model_5 <- keras_model_sequential() %>%
  layer_conv_2d(filters = 32,  kernel_size = c(5, 5), activation = "relu", input_shape = c(64, 64, 3)) %>%
  layer_conv_2d(filters = 32,  kernel_size = c(5, 5), activation = "relu") %>% 
  layer_batch_normalization(epsilon = 0.01) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 64, kernel_size = c(5, 5), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 128, kernel_size = c(5, 5), activation = "relu") %>%
  layer_batch_normalization(epsilon = 0.01) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.1) %>%
  layer_dense(units = 256, activation = "relu") %>%
  layer_dropout(rate = 0.5) %>%
  layer_dense(units = 512, activation = "relu") %>%
  layer_dense(units = 2, activation = "softmax")

model_5 %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_rmsprop(),
  metrics = c('accuracy')
)

```




```{r}
NEXEC = 5
loss <- 0
precision <- 0
time <- 0

for (i in 1:NEXEC){
 start_model_5 <- Sys.time()
  history <- model_5 %>% 
  fit_generator(
    generator = train_generator_flow_4, 
    validation_data = validation_generator_flow_4,
    steps_per_epoch = 10,
    epochs = 20
  )
  
  
  end_model_5 <- Sys.time()
  plot(history)
  
  metrics <- model_5 %>% 
    evaluate_generator(test_generator_flow_4, steps = 5)
  
  time <- time + (end_model_5 - start_model_5)
  precision <- precision + as.numeric(metrics[2])
  loss <- loss + as.numeric(metrics[1])
  message("Iteracion: ",i )
  message("\t Tiempo: ", specify_decimal((end_model_5 - start_model_5), 2))
  message("\tPrecisión: ", specify_decimal(as.numeric(metrics[2]), 2))
  message("\tPerdida: ", specify_decimal(as.numeric(metrics[1]), 2))
}

time <- time / NEXEC
precision <- precision /NEXEC
loss <- loss / NEXEC

message("Total: ",i )
message("\t Tiempo: ", specify_decimal(time,2))
message("\tPrecisión: ", specify_decimal(as.numeric(precision),2))
message("\tPerdida: ", specify_decimal(as.numeric(loss),2))

```
